Recognition by peers or the general public can greatly affect the trajectory of a scientist’s career since each form of recognition paves the way to future accolades.
  Regardless of an individual’s merit, biases can skew which scientists are considered for recognition.
  Audits can combat this by identifying neglected groups and hold the recognizing body accountable.

Auditing efforts have shown that they can help reduce disparities.
  In science journalism, Adrienne LaFrance, supported by Nathan Matias, and Ed Yong performed self-audits to quantify gender disparity in their sources [@https://www.theatlantic.com/technology/archive/2016/02/gender-diversity-journalism/463023/; @https://www.theatlantic.com/science/archive/2018/02/i-spent-two-years-trying-to-fix-the-gender-imbalance-in-my-stories/552404/].
  While a secondary audit performed by LaFrance showed no improvement, Yong, through continuous self-auditing, was able to achieve gender parity in his reporting.

In award recognition, an audit of the International Society for Computational Biology (ISCB) honorees revealed a significant under-representation of people with East Asian name origins and over-representation of US-affiliated scientists.
  After this study was available to the public, the next honorees had the highest mean predicted probability of having an East Asian name of any previous year.
  Additionally, the nominating committee inducted the first China-based fellow [@doi:10.1016/j.cels.2021.07.007].

Audits can also provide evidence for specific actions.
  A recent study of disparities in scientific journalism found that predicted East Asian names were under-quoted and under-mentioned [@doi:10.1101/2021.06.21.449261]. 
  However, this disparity was almost completely removed when considering quoted or mentioned people with a US-affiliated paper cited in the article.
  This suggests a regional bias, arguing for more regionally focused or co-located journalists.

Audits of representation would ideally be done with all individuals self-reporting gender, ethnicity, and other identifications [@doi:10.1371/journal.pcbi.1003903].
  While this may be possible prospectively, surveying for self-identification is impractical for large groups and often impossible retrospectively [@http://mediashift.org/2014/11/how-to-ethically-and-responsibly-identify-gender-in-large-datasets/].
  Computationally derived predictions allow for audits on a scale that would not be possible otherwise.
  Numerous tools exist to algorithmically infer gender, nationality, and ethnicity information using only the feature most likely to be non-missing in a relevant dataset: an individual's name. [@doi:10.1093/pan/mpw001; @http://genderize.io/]
  Most of these models are highly scalable, allowing auditors to define the scope of their target group and background population as broadly as needed.
  
Prediction models are not a panacea; several factors limit both their accuracy at recapitulating self-reported information and their ability to address the underlying motivating questions of diversity audits.
  For instance, gender associations of a given name can vary by culture, potentially biasing gender predictions where additional information is not available [@https://aclanthology.org/U14-1021].
  Also, most gender prediction models are trained on binary gender labels, which occludes assessing the representation of transgender, non-binary, and intersex individuals [@doi:10.1101/2020.10.12.336230].
 
Proxy predictions of ethnicity via name origin are more difficult still; choosing categories to probabilistically predict on is non-trivial and difficult to define. 
  Furthermore, there is no one-to-one mapping between having a name from a linguistic group and belonging to a minoritized or underrepresented group.
  Colonialism, immigration, and structural racism have affected most groups' linguistic history and inclusion or exclusion from scientific communities in complex ways that are nearly impossible to parse from names alone. 
  For instance, classifiers are usually unable to distinguish if names of Hispanic origin come from the Iberian Peninsula or from Latin America [@doi:10.1016/j.cels.2021.07.007]. 
  If a target and background population had identical probabilistic proportions of Hispanic names, but the target group primarily consisted of Iberian individuals, underrepresentation of Latin American scientists would go unnoticed in an exclusively computational analysis.

Recognizing the aforementioned shortcomings, we propose the following recommendations for the creation and deployment of automated auditing tools:
  1. *Transparency.* Publicly provide all tools, code, and data used in the analysis.
  This enables public scrutiny to those being audited and those whose data you are using.
  For confidential data, we recommend providing de-identified or aggregated data. 

  2. *Individuals know best.* Self-identified demographic information should be used in preference to algorithmic predictions.

  3. *Aggregates only.* Audit results should not affect individuals.
  Analyses must focus on aggregate estimations and any intermediary individual predictions should not be used externally.
  In addition, analysts must be mindful of hidden subpopulations that may be obscured during aggregation.

  4. *Inform the public.* While internal audits can help institutions reflect, the public must be given an opportunity to hold them accountable.

