Recognition by peers or the general public can greatly affect the trajectory of a scientist’s career.
  Any recognition--inclusion of their name in print as an expert, an invited lecture, or an award--paves the way to future accolades.
  Regardless of an individual’s merit, biases can skew which scientists are even considered for recognition.
  To combat biases, audits can identify if specific groups are being unintentionally neglected and hold the recognizing body accountable.

The use of auditing methods has produced evidence that they can help reduce disparities.
  In science journalism, Adrienne LaFrance, supported by Nathan Matias, and Ed Yong performed self-audits to quantify gender disparity in their quoted or mentioned sources [@https://www.theatlantic.com/technology/archive/2016/02/gender-diversity-journalism/463023/; @https://www.theatlantic.com/science/archive/2018/02/i-spent-two-years-trying-to-fix-the-gender-imbalance-in-my-stories/552404/].
  While a secondary audit performed by LaFrance showed no improvement from her initial audit, Yong, through continuous self-auditing, was able to achieve gender parity in his reporting.

In award recognition, an audit of the International Society for Computational Biology (ISCB) honorees revealed a significant disparity against people with East Asian name origins and towards US-affiliated scientists.
  After this study was made available to the public, the following set of honorees had the highest mean predicted probability of an honoree having an East Asian name of any previous year.
  Additionally, the nominating committee inducted the first China-based fellow into ISCB [@doi:10.1016/j.cels.2021.07.007].

Audits can also provide evidence for specific corrective actions.
  In a recent study of disparities in scientific journalism, it was found that people with an East Asian name origin were under-quoted and under-mentioned [@doi:10.1101/2021.06.21.449261]. 
  However, this disparity was almost completely removed by filtering down to US-affiliated papers cited.
  This implies that source gathering beyond authors in publications may be regionally biased, arguing for more regionally focused or co-located journalists.

Audits of representation would ideally be done with all individuals self-reporting gender, ethnicity, and other identifications [@doi:10.1371/journal.pcbi.1003903].
  While this may be possible prospectively, surveying for self-identification is impractical for large groups and often impossible retrospectively [@http://mediashift.org/2014/11/how-to-ethically-and-responsibly-identify-gender-in-large-datasets/].
  Computationally derived predictions allow for audits on a scale that would not be possible otherwise.
  Numerous tools exist to algorithmically infer gender, nationality, and ethnicity information using only the feature most likely to be non-missing in a relevant dataset: an individual's name. [@doi:10.1093/pan/mpw001; @http://genderize.io/]
  Most of these models are highly scalable, allowing auditors to define the scope of their target group and background population as broadly as needed.
  
Prediction models are not a panacea; several factors limit both their accuracy at recapitulating self-reported information and their ability to address the underlying motivating questions of diversity audits.
  For instance, gender associations of a given name can vary by culture, potentially biasing gender predictions where additional information is not available [@https://aclanthology.org/U14-1021].
  Also, most gender prediction models are trained on binary gender labels, which occludes assessing the representation of transgender, non-binary, and intersex individuals [@doi:10.1101/2020.10.12.336230].
 
Proxy predictions of ethnicity via name origin are more difficult still; choosing categories to probabilistically predict on is non-trivial and difficult to define. 
  Furthermore, there is no one-to-one mapping between having a name from a linguistic group and belonging to a minoritized or underrepresented group.
  Colonialism, immigration, and structural racism have affected most groups' linguistic history and inclusion or exclusion from scientific communities in complex ways that are nearly impossible to parse from names alone. 
  For instance, classifiers are usually unable to distinguish if names of Hispanic origin come from the Iberian Peninsula or from Latin America [@doi:10.1016/j.cels.2021.07.007]. 
  If a target and background population had identical probabilistic proportions of Hispanic names, but the target group primarily consisted of Iberian individuals, underrepresentation of Latin American scientists would go unnoticed in an exclusively computational analysis.

Recognizing the aforementioned shortcomings, we propose the following recommendations for the creation and deployment of automated auditing tools:
  1. *Transparency.* Publicly provide all tools, code, and data used in the analysis.
  This enables public scrutiny and transparency to those being audited and those whose data you are using.
  If the data is private, we recommend providing de-identified or aggregated data. 

  2. *Individuals know best.* Self-identified demographic information should be used in preference to automated predictions.

  3. *Aggregates only.* Audit results should not affect individuals.
  The analyses must focus only on aggregate estimations and any intermediary predictions on an individual should not be used independently.
  In addition, analysts must be mindful of hidden subpopulations that may be obscured when calculating aggregate statistics.

  4. *Inform the public.* While internal audits can help institutions monitor their practices, the key goal is to remove disparities in public-facing forms of representation. 
  We believe transparency and the ability to track progress over time is the most effective method to achieve this.
