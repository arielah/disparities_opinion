Recognition by peers or the general public can greatly affect the trajectory of a scientist’s career.
  Whether the recognition is the inclusion of their name in print as an expert, an invited lecture, or an award, each form of recognition paves the way to future accolades.
  Regardless of an individual’s merit, biases can skew which scientists are even considered for recognition.
  To combat biases, auditing systems can identify if specific groups are being unintentionally neglected and hold the recognizing body accountable.

The use of auditing methods has produced evidence that they can help reduce disparities.
  In science journalism, Adrienne LaFrance, supported by Nathan Matias, and Ed Yong performed self-audits to quantify gender disparity in their quoted or mentioned sources [@https://www.theatlantic.com/technology/archive/2016/02/gender-diversity-journalism/463023/; @https://www.theatlantic.com/science/archive/2018/02/i-spent-two-years-trying-to-fix-the-gender-imbalance-in-my-stories/552404/].
  While a secondary audit performed by LaFrance showed no improvement from her initial audit, Yong, through continuous self-auditing, was able to achieve gender parity in his reporting.

In award recognition, an audit of the International Society for Computational Biology (ISCB) honorees revealed a significant disparity against people with East Asian name origins and towards US-affiliated scientists.
  After this study was made available to the public, the following set of honorees had the highest mean predicted probability of an honoree having an East Asian name of any previous year.
  Additionally, it inducted the first China-based fellow into ISCB [@doi:10.1016/j.cels.2021.07.007].

Computationally derived predictions allow for diversity audits on a scale that would not be possible otherwise.
  Numerous tools exist to algorithmically infer gender, nationality, and ethnicity information using only the feature most likely to be non-missing in a relevant dataset: an individual's name. <decide what to cite here based on how many sources we have left>
  Most of these models are highly scalable, allowing auditors to define the scope of their target group and background population as broadly as needed.
  
Prediction models are not a panacea; several factors limit both their accuracy at recapitulating self-reported information and their ability to address the underlying motivating questions of diversity audits.
  For an example of the former, gender associations of a given name can vary by culture, potentially biasing gender predictions where additional information is not available [@https://aclanthology.org/U14-1021].
  For an example of the latter, most gender prediction models are trained on binary gender labels, which occludes assessing the representation of transgender, non-binary, and intersex individuals [@doi:10.1101/2020.10.12.336230].
 
Proxy predictions of ethnicity via name origin are more difficult still; the choice of categories to probabilistically predict on is non-trivial and difficult to define. 
  Furthermore, there is no one-to-one mapping between having a name from a linguistic group and belonging to a minoritized or underrepresented group.
  Colonialism, immigration, and structural racism have affected most groups' linguistic history and inclusion or exclusion from scientific communities in complex ways that are nearly impossible to parse from names alone. 
  For instance, these classifiers are usually unable to distinguish if names of Hispanic origin come from the Iberian Peninsula or from Latin America [@doi:10.1016/j.cels.2021.07.007]. 
  If a target and background population had identical probabilistic proportions of Hispanic names, but the target group primarily consisted of individuals from Spain and Portugal, underrepresentation of Latin American scientists would then go unnoticed in an exclusively computational analysis.

Recognizing the aforementioned shortcomings, we propose the following recommendations for the creation and deployment of automated auditing tools:
  1. *Transparency.* Publicly provide all tools, code, and data used in the analysis.
  This is to enable public scrutiny and transparency to those being audited and those whose data you are using.
  If the data is private, we recommend providing de-identified or aggregated data. 

  2. *Individuals know best.* Self-identified demographic information should be used in preference to automated predictions.
  Additionally, use the most granular and representative groups possible.

  3. *Aggregates only.* Audit results should not affect individuals.
  Gender expression and ethnicity of an individual are not for an algorithm to decide nor are they the focus of an auditing system. 
  The analyses must focus only on aggregate estimations and any intermediary predictions on an individual should not be used independently.

  4. *Inform the public.* While internal audits can help institutions monitor their practices, the key goal is to remove disparities in public-facing forms of representation. 
  We believe transparency to the public along with a way to track progress over time is the most effective method to achieve this.